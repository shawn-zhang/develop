
Android media struct

framework/av/

1. camera
2. media
3. services
4. include

1. camera
   camera作为一个独立的media模块单独对外提供接口，并没有集成在libmedia.so中，他会独立编译成libcamera.so,对外提供接口
   ICameraClient.cpp  ICamera.cpp  ICameraRecordingProxy.cpp  ICameraRecordingProxyListener.cpp  ICameraService.cpp  ICameraServiceListener.cpp  IProCameraCallbacks.cpp  IProCameraUser.cpp
2. services
   services目录包括了当前media模块除了mediplayerService之外的其他所有的独立service，包括audioflinger, cameraService, audiopolicyservice,mediaLog等服务.
3. media
   media作为一个非常复杂的模块，其内部有分成了几个子模块，以方便解偶, 其内部也包含了audioflinger的对外接口，对外，除了camera的即可，都是由libmedia.so对外提供接口。
1). libmedia
2). libmediaplayerservice
3). libstagefright

1). libmedia目录
   libmedia.so是整个底层media实现对外的接口，mediaplayer都是通过libmedia.so与mediaPlayerService进行通信。libmedia中定义并实现了所有的IMedia*的Binder接口，提供给MediaPlayer或者其他模块使用。
   IAudioFlingerClient.cpp        IAudioRecord.cpp  IDrm.cpp           IMediaCodecList.cpp       IMediaLogService.cpp         IMediaPlayerService.cpp   IRemoteDisplayClient.cpp
   IAudioFlinger.cpp              IAudioTrack.cpp   IEffectClient.cpp  IMediaDeathNotifier.cpp   IMediaMetadataRetriever.cpp  IMediaRecorderClient.cpp  IRemoteDisplay.cpp
   IAudioPolicyServiceClient.cpp  ICrypto.cpp       IEffect.cpp        IMediaHTTPConnection.cpp  IMediaPlayerClient.cpp       IMediaRecorder.cpp        IStreamSource.cpp
   IAudioPolicyService.cpp        IDrmClient.cpp    IHDCP.cpp          IMediaHTTPService.cpp     IMediaPlayer.cpp             IOMX.cpp

   IMediaMetaDataRetiever: 通过MediaPlayerService提取(Retriever)MetaData，也就是封装格式的解析.实现类是libmedia/MediaMetaDataRetriever.cpp
   IAudioFlinger.cpp:      AudioFlinger对外提供接口。
   IAudioPolicyService.cpp: AudioPolicyService对外部提供接口，比如获取音频out通道。
   IAudioRecord.cpp, IAudioTrack.cpp 音频录制和播放接口，服务是AudioFlinger

2). libMediaPlayerService
   系统中MediaPlayer的具体实现，逻辑的控制。
3). libstagefright
   目前Android中使用的播放框架，类似ffmpeg,执行具体的播控逻辑，挂载解码库等。

4). openMax
   OpenMax分了三个层次，分别是AL(Application Layer), IL(Integer Layer), DL(Develop Layer). Android中主要使用了其中的IL层，也就是集成层。Andorid 中的openMax可以单独调用其接口，可以通过mediaplayerService获取openMax的BpBinder来调用真正的binder, openMax的具体调用过程，也就是Android中适配的openMax IL层的具体实现实在libstagefright.so中，openMax IL层的具体接口可以查看omx.h，omx.cpp是具体实现。具体实现暂时不表。
   那么我们调用openMax IL层的逻辑顺序是什么呢，如果调用openMax IL层的接口呢。通过Android我们具体分析。
   Android为了调用方便，在stagefright中增加了OmxCodec.h的接口，这个接口就是调用openMax的Android中的接口，屏蔽了IL层的具体调用过程.
   1. 创建OmxClient，openMax为了调用方便，创建OmxClient实例。
   2. 在使用OmxClient之前，首先与server端建立连接，OMXClient.connect();
   3. status_t err = omx->allocateNode(componentName, observer, &node);根据要使用的组件的名字，创建具体的使用实例，实际上就是个handle，用这个handle来代替这个组件使用中的相关上下文。
   4. OMXCodec::setComponentRole(omx, node, isEncoder, mime); 创建实例之后我们可以对这个实例的一些属性或者工作状态进行一些设定，这个地方就是设置这个实例(组件的handle)的具体角色，也就是干什么用的。比如，用这个组件处理那种类型的音视频格式，是编码还是解码。
   5. err = omx->getParameter( node, OMX_IndexParamVideoProfileLevelQuerySupported, &param, sizeof(param)); 
      err = omx->getParameter( node, OMX_IndexParamVideoPortFormat,&portFormat, sizeof(portFormat));
      获取解码器或者编码器的相关参数，具体可获取参数的列表参见OMX_Index.h
      typedef enum OMX_INDEXTYPE;
   6. err = mOMX->setParameter(mNode, OMX_IndexParamAudioAac, &profile, sizeof(profile));
   7. ... 其他功能接口的调用
   8. omx->freeNode(node)  释放node，完成操作。
   综上所述，我们在Android中调用openMax的接口，不需要调用真正的openMax的那些接口，我们直接调用OMXCodec.h中的接口即可。
5). err = mOMX->getParameter(mNode, OMX_IndexParamPortDefinition, &def, sizeof(def));
   获取组件中对于端口的定义，OpenMax IL中共有四种组件，分别是Source组间，Host组件，Decoder组件, Sink组件。
   那么我们在Android中只是使用了Decoder组件，所以这个地方mNode就是我们之前创建的那个节点的组件。Decoder组件是有两个端口，分别是输入数据的端口和输出数据的端口。这个地方可以获取decoder中对于输入端口的buffer的定义的大小和输出组件中对于buffer的定义的大小。
   另外，我们给decoder组件申请的输入buffer时间上是Source组件的输出buffer,因为decoder解码的时候需要输入数据，所以从decoder中申请好buffer之后，直接通过Source->setBuffers的接口设置给source组件就可以了。
   status_t err = mSource->setBuffers(buffers);

2015.12.10  21:55
   解释一下seek的模式,stagefright中对于seek模式有几种模式
   enum SeekMode {
       SEEK_PREVIOUS_SYNC,
       SEEK_NEXT_SYNC,
       SEEK_CLOSEST_SYNC,
       SEEK_CLOSEST,
   };
   当我们在播放界面上做了一个拖动进度条的操作，放手的那一刻我们最终给代码传递了一个值：我们拖动到的视频的播放的时刻，这里以代码中的seekTimeUs表示，得到这个值之后，我们大致的流程就是通过这个值，获取对应的sampleIndex，得到sampleIndex之后我们找离这个sampleIndex附近的某一关键帧的syncSampleIndex(附近的判定原则有三种，向前，向后，前后最近，对应三个case)，得到syncSampleIndex之后，得到这个sampleIndex对应的offset和size，cts等信息，就可以送到解码器去播放。

2015.12.16 14:32
  关于Stagefrightplayer和NuPlayer的不同，stagefrightplayer其实是awesomplayer，整个实现的逻辑也就是都在awesomeplayer中，awesomplayer中会整合mediaplayer的mediaextor, omxcodec, openmax的调用，然后根据时间戳来进行音视频同步，包括视频的渲染，视频的渲染相对来说比较简单。在创建awesomplayer之后，如果我们需要将视频内容显示到屏幕上，那么我们需要从surfacefligner去申请graphicbuffer才可以，到时候直接就会合成到framebuffer上了。
  通常的接口调用可以参考stream.cpp中的调用。
  在omxcodec的实现中，如果输出的buffer使用的是graphicbuffer，那么我们需要按照openmax的要求来从graphicBuffer中申请足够的buffer，并把这些buffer的信息包括地址都告诉openmax，openmax在解码完成后直接使用输出到这些buffer中，那么我们可以从openmax中直接读取到解码后的数据buffer，直接推送给surfaceflinger即可。
  在awesomeplayer中，直接调用omxcodec然后使用openMax进行解码，nuplayer中使用mediacodec进行解码，相对来说，nuplayer更规范些，这样可以少维护一套代码，因为mediacodec是android对外服务的一套标准接口.


2016.6.12 18:34

1. WifiDisplay的结构
   system_server启动的时候会注册WifiDisplayAdapter, WifiDisplayAdapter继承自DisplayAdapter, WifiDisplayAdapter会管理整个WifiDisplay的状态。
   WifiDisplayAdapter会创建WifiDisplayController, WifiDisplayController会进行整个WifiP2p,Rtsp等整个控制过程。
   WifiDisplaySettings只会与WifiDisplayAdapter进行控制，交互，并且需要通过DisplayManagerService进行接口的调用。

   WifiDisplayController会通过native创建RemoteDisplay实例，RmoteDisplay最终会在MediaPlayerService中创建.
   RemoteDisplay比较简单，其实就是WifiDisplaySource的wrapper类。
   RemoteDisplay会建立WifiDisplaySource的对象，WifiDisplaySource会包括playbackSession(encoder, muxer), rtsp(NetworkSession)等一系列Wifidiaplay的管理。
   RTSP部分的管理是由ANetworkSession实现管理。
   Encoder和Muxer部分由PlaybackSession来管理实现。
   同时SurfaceTexure也是由PlaybackSession在创建Encoder的时候创建，是Encoder的输入部分，也就是Productor.
   输出部分的数据由PlaybackSession直接管理，通过MediaSender建立TsPacket，并通过RTP Sender发送到对端， RTP的建立由RTP sender通过ANetworkSession建立， 编码器编好的数据调用MediaSender的queueAccessUnit方法即可。
   并且WifiDisplaySource会通过PlaybackSession获取到Productor, 通过Productor转换成surface，并将sureface handle传至WifidiaplayAdapter，然后createDisplay, 设置diaplay的surface到surfaceFlinger中。

2. 关于source sink的RTSP的建立。
   协议规定，Source端建立RTSP Server, Sink连接。RTSP的端口可以通过Mirrcast的WifiDirect来确定，如果没有确定，使用默认7236端口
   RTSP连接建立之后，可以通过GET_PARAMETERS和SET_PARAMETERS进行交互，确定RTP的端口，RTP的Server由RTSP的Client端建立，然后在Setup时候建立PlaybackSession，并建立RTP连接。
   由于Mirracast的RTSP是点对点的，所以，跟标准的RTSP协议交互上有很大的不同。

2016.7.13

    audio_policy.conf

audio_hw_modules {
  primary {                                      #### hwModule->name                    
    outputs {
      primary {                                  #### hwModule->IOProfile->name    IOProfile(thread)
        sampling_rates 44100|48000
        channel_masks AUDIO_CHANNEL_OUT_STEREO
        formats AUDIO_FORMAT_PCM_16_BIT
        devices AUDIO_DEVICE_OUT_EARPIECE|AUDIO_DEVICE_OUT_SPEAKER|AUDIO_DEVICE_OUT_WIRED_HEADSET|AUDIO_DEVICE_OUT_WIRED_HEADPHONE|AUDIO_DEVICE_OUT_LINE|AUDIO_DEVICE_OUT_ALL_SCO|AUDIO_DEVICE_OUT_AUX_DIGITAL|AUDIO_DEVICE_OUT_PROXY|AUDIO_DEVICE_OUT_FM
        flags AUDIO_OUTPUT_FLAG_FAST|AUDIO_OUTPUT_FLAG_PRIMARY
      }
      raw {                                    #### hwModule->IOProfile->name       IOProfile(thread)
        sampling_rates 48000
        channel_masks AUDIO_CHANNEL_OUT_STEREO
        formats AUDIO_FORMAT_PCM_16_BIT
        devices AUDIO_DEVICE_OUT_EARPIECE|AUDIO_DEVICE_OUT_SPEAKER|AUDIO_DEVICE_OUT_WIRED_HEADSET|AUDIO_DEVICE_OUT_WIRED_HEADPHONE|AUDIO_DEVICE_OUT_LINE|AUDIO_DEVICE_OUT_ALL_SCO|AUDIO_DEVICE_OUT_AUX_DIGITAL|AUDIO_DEVICE_OUT_PROXY
        flags AUDIO_OUTPUT_FLAG_FAST|AUDIO_OUTPUT_FLAG_RAW
      }
      deep_buffer {
         sampling_rates 44100|48000
         channel_masks AUDIO_CHANNEL_OUT_STEREO
         formats AUDIO_FORMAT_PCM_16_BIT
         devices AUDIO_DEVICE_OUT_SPEAKER|AUDIO_DEVICE_OUT_EARPIECE|AUDIO_DEVICE_OUT_WIRED_HEADSET|AUDIO_DEVICE_OUT_WIRED_HEADPHONE|AUDIO_DEVICE_OUT_LINE|AUDIO_DEVICE_OUT_ALL_SCO|AUDIO_DEVICE_OUT_AUX_DIGITAL|AUDIO_DEVICE_OUT_PROXY|AUDIO_DEVICE_OUT_FM
         flags AUDIO_OUTPUT_FLAG_DEEP_BUFFER
      }
      multichannel {
        sampling_rates 8000|11025|16000|22050|32000|44100|48000|64000|88200|96000|128000|176400|192000
        channel_masks dynamic
        formats AUDIO_FORMAT_PCM_16_BIT
        devices AUDIO_DEVICE_OUT_AUX_DIGITAL|AUDIO_DEVICE_OUT_PROXY
        flags AUDIO_OUTPUT_FLAG_DIRECT
      }


2016.7.21
   qcom wifidisplay

   1. 两个服务，一个是WfdService.java 和 SessionManagerService.java 组成一个java层服务，用于上呈下达，与WifiDisplayService建立连接。
   2. WifiDisplayService里面重要的模块有：
      1. RTSPSession， 管理RTSP状态机
      2. WFDMMSource,  里面管理了所有source端的组件，包括muxer, videoSource, AudioSource.
      3. VideoSource,  包括WFDMMSourceVideoEncode
      4. VideoEncode,  调用openMax进行encoder编码，以及WFDMMSourceVideoCapture.
      5. WFDMMSourceVideoCapture, 与surfaceflinger交互，抓取surfaceflinger生成的数据。
      6. Muxer, WFDMMSourceMux和omx_filemux.cpp(wfd/mm/omx-mux/) qcom将muxer也封装成openMax的组件，WFDMMSourceMux将会调用openMax的标准接口使用OMX.qcom.file.muxer的内容。OMX.qcom.file.muxer将会接受数据并接受配置，调用qcom的公共库libFileMux进行TS流的数据packet, 完成之后调用libmmrtpencoder进行RTP的数据传输。









