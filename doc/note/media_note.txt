
Android media struct

framework/av/

1. camera
2. media
3. services
4. include

1. camera
   camera作为一个独立的media模块单独对外提供接口，并没有集成在libmedia.so中，他会独立编译成libcamera.so,对外提供接口
   ICameraClient.cpp  ICamera.cpp  ICameraRecordingProxy.cpp  ICameraRecordingProxyListener.cpp  ICameraService.cpp  ICameraServiceListener.cpp  IProCameraCallbacks.cpp  IProCameraUser.cpp
2. services
   services目录包括了当前media模块除了mediplayerService之外的其他所有的独立service，包括audioflinger, cameraService, audiopolicyservice,mediaLog等服务.
3. media
   media作为一个非常复杂的模块，其内部有分成了几个子模块，以方便解偶, 其内部也包含了audioflinger的对外接口，对外，除了camera的即可，都是由libmedia.so对外提供接口。
1). libmedia
2). libmediaplayerservice
3). libstagefright

1). libmedia目录
   libmedia.so是整个底层media实现对外的接口，mediaplayer都是通过libmedia.so与mediaPlayerService进行通信。libmedia中定义并实现了所有的IMedia*的Binder接口，提供给MediaPlayer或者其他模块使用。
   IAudioFlingerClient.cpp        IAudioRecord.cpp  IDrm.cpp           IMediaCodecList.cpp       IMediaLogService.cpp         IMediaPlayerService.cpp   IRemoteDisplayClient.cpp
   IAudioFlinger.cpp              IAudioTrack.cpp   IEffectClient.cpp  IMediaDeathNotifier.cpp   IMediaMetadataRetriever.cpp  IMediaRecorderClient.cpp  IRemoteDisplay.cpp
   IAudioPolicyServiceClient.cpp  ICrypto.cpp       IEffect.cpp        IMediaHTTPConnection.cpp  IMediaPlayerClient.cpp       IMediaRecorder.cpp        IStreamSource.cpp
   IAudioPolicyService.cpp        IDrmClient.cpp    IHDCP.cpp          IMediaHTTPService.cpp     IMediaPlayer.cpp             IOMX.cpp

   IMediaMetaDataRetiever: 通过MediaPlayerService提取(Retriever)MetaData，也就是封装格式的解析.实现类是libmedia/MediaMetaDataRetriever.cpp
   IAudioFlinger.cpp:      AudioFlinger对外提供接口。
   IAudioPolicyService.cpp: AudioPolicyService对外部提供接口，比如获取音频out通道。
   IAudioRecord.cpp, IAudioTrack.cpp 音频录制和播放接口，服务是AudioFlinger

2). libMediaPlayerService
   系统中MediaPlayer的具体实现，逻辑的控制。
3). libstagefright
   目前Android中使用的播放框架，类似ffmpeg,执行具体的播控逻辑，挂载解码库等。

4). openMax
   OpenMax分了三个层次，分别是AL(Application Layer), IL(Integer Layer), DL(Develop Layer). Android中主要使用了其中的IL层，也就是集成层。Andorid 中的openMax可以单独调用其接口，可以通过mediaplayerService获取openMax的BpBinder来调用真正的binder, openMax的具体调用过程，也就是Android中适配的openMax IL层的具体实现实在libstagefright.so中，openMax IL层的具体接口可以查看omx.h，omx.cpp是具体实现。具体实现暂时不表。
   那么我们调用openMax IL层的逻辑顺序是什么呢，如果调用openMax IL层的接口呢。通过Android我们具体分析。
   Android为了调用方便，在stagefright中增加了OmxCodec.h的接口，这个接口就是调用openMax的Android中的接口，屏蔽了IL层的具体调用过程.
   1. 创建OmxClient，openMax为了调用方便，创建OmxClient实例。
   2. 在使用OmxClient之前，首先与server端建立连接，OMXClient.connect();
   3. status_t err = omx->allocateNode(componentName, observer, &node);根据要使用的组件的名字，创建具体的使用实例，实际上就是个handle，用这个handle来代替这个组件使用中的相关上下文。
   4. OMXCodec::setComponentRole(omx, node, isEncoder, mime); 创建实例之后我们可以对这个实例的一些属性或者工作状态进行一些设定，这个地方就是设置这个实例(组件的handle)的具体角色，也就是干什么用的。比如，用这个组件处理那种类型的音视频格式，是编码还是解码。
   5. err = omx->getParameter( node, OMX_IndexParamVideoProfileLevelQuerySupported, &param, sizeof(param)); 
      err = omx->getParameter( node, OMX_IndexParamVideoPortFormat,&portFormat, sizeof(portFormat));
      获取解码器或者编码器的相关参数，具体可获取参数的列表参见OMX_Index.h
      typedef enum OMX_INDEXTYPE;
   6. err = mOMX->setParameter(mNode, OMX_IndexParamAudioAac, &profile, sizeof(profile));
   7. ... 其他功能接口的调用
   8. omx->freeNode(node)  释放node，完成操作。
   综上所述，我们在Android中调用openMax的接口，不需要调用真正的openMax的那些接口，我们直接调用OMXCodec.h中的接口即可。
5). err = mOMX->getParameter(mNode, OMX_IndexParamPortDefinition, &def, sizeof(def));
   获取组件中对于端口的定义，OpenMax IL中共有四种组件，分别是Source组间，Host组件，Decoder组件, Sink组件。
   那么我们在Android中只是使用了Decoder组件，所以这个地方mNode就是我们之前创建的那个节点的组件。Decoder组件是有两个端口，分别是输入数据的端口和输出数据的端口。这个地方可以获取decoder中对于输入端口的buffer的定义的大小和输出组件中对于buffer的定义的大小。
   另外，我们给decoder组件申请的输入buffer时间上是Source组件的输出buffer,因为decoder解码的时候需要输入数据，所以从decoder中申请好buffer之后，直接通过Source->setBuffers的接口设置给source组件就可以了。
   status_t err = mSource->setBuffers(buffers);

2015.12.10  21:55
   解释一下seek的模式,stagefright中对于seek模式有几种模式
   enum SeekMode {
       SEEK_PREVIOUS_SYNC,
       SEEK_NEXT_SYNC,
       SEEK_CLOSEST_SYNC,
       SEEK_CLOSEST,
   };
   当我们在播放界面上做了一个拖动进度条的操作，放手的那一刻我们最终给代码传递了一个值：我们拖动到的视频的播放的时刻，这里以代码中的seekTimeUs表示，得到这个值之后，我们大致的流程就是通过这个值，获取对应的sampleIndex，得到sampleIndex之后我们找离这个sampleIndex附近的某一关键帧的syncSampleIndex(附近的判定原则有三种，向前，向后，前后最近，对应三个case)，得到syncSampleIndex之后，得到这个sampleIndex对应的offset和size，cts等信息，就可以送到解码器去播放。

2015.12.16 14:32
  关于Stagefrightplayer和NuPlayer的不同，stagefrightplayer其实是awesomplayer，整个实现的逻辑也就是都在awesomeplayer中，awesomplayer中会整合mediaplayer的mediaextor, omxcodec, openmax的调用，然后根据时间戳来进行音视频同步，包括视频的渲染，视频的渲染相对来说比较简单。在创建awesomplayer之后，如果我们需要将视频内容显示到屏幕上，那么我们需要从surfacefligner去申请graphicbuffer才可以，到时候直接就会合成到framebuffer上了。
  通常的接口调用可以参考stream.cpp中的调用。
  在omxcodec的实现中，如果输出的buffer使用的是graphicbuffer，那么我们需要按照openmax的要求来从graphicBuffer中申请足够的buffer，并把这些buffer的信息包括地址都告诉openmax，openmax在解码完成后直接使用输出到这些buffer中，那么我们可以从openmax中直接读取到解码后的数据buffer，直接推送给surfaceflinger即可。
  在awesomeplayer中，直接调用omxcodec然后使用openMax进行解码，nuplayer中使用mediacodec进行解码，相对来说，nuplayer更规范些，这样可以少维护一套代码，因为mediacodec是android对外服务的一套标准接口.


2016.6.12 18:34

1. WifiDisplay的结构
   system_server启动的时候会注册WifiDisplayAdapter, WifiDisplayAdapter继承自DisplayAdapter, WifiDisplayAdapter会管理整个WifiDisplay的状态。
   WifiDisplayAdapter会创建WifiDisplayController, WifiDisplayController会进行整个WifiP2p,Rtsp等整个控制过程。
   WifiDisplaySettings只会与WifiDisplayAdapter进行控制，交互，并且需要通过DisplayManagerService进行接口的调用。

   WifiDisplayController会通过native创建RemoteDisplay实例，RmoteDisplay最终会在MediaPlayerService中创建，RemoteDisplay会建立WifiDisplaySource的对象，WifiDisplaySource会包括encoder, muxer,rtsp等一系列Wifidiaplay的管理。
   并且WifiDisplaySource会创建surface，并将sureface handle传至WifidiaplayAdapter，然后createDisplay, 设置diaplay的surface







